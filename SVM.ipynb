{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import sys;\n",
    "import pickle;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_CIFAR10(ROOT):\n",
    "    \"\"\"\n",
    "    load entire CIFAR-10 dataset\n",
    "    \n",
    "    code is adapted from CS231n assignment kit\n",
    "    \n",
    "    @param ROOT: string of data folder\n",
    "    @return: Xtr, Ytr: training data and labels\n",
    "    @return: Xte, Yte: testing data and labels\n",
    "    \"\"\"\n",
    "    \n",
    "    xs=[];\n",
    "    ys=[];\n",
    "    \n",
    "    for b in range(1,6):\n",
    "        f=os.path.join(ROOT, \"data_batch_%d\" % (b, ));\n",
    "        X, Y=load_CIFAR_batch(f);\n",
    "        xs.append(X);\n",
    "        ys.append(Y);\n",
    "        \n",
    "\n",
    "    Xtr=np.concatenate(xs);\n",
    "    Ytr=np.concatenate(ys);\n",
    "    del X, Y;\n",
    "    \n",
    "    Xte, Yte=load_CIFAR_batch(os.path.join(ROOT, \"test_batch\"));\n",
    "    \n",
    "    return Xtr, Ytr, Xte, Yte;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_CIFAR_batch(filename):\n",
    "    \"\"\"\n",
    "    load single batch of cifar-10 dataset\n",
    "    \n",
    "    code is adapted from CS231n assignment kit\n",
    "    \n",
    "    @param filename: string of file name in cifar\n",
    "    @return: X, Y: data and labels of images in the cifar batch\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        if sys.version_info.major == 2:\n",
    "            datadict = pickle.load(f)\n",
    "        elif sys.version_info.major == 3:\n",
    "            datadict = pickle.load(f, encoding='latin-1')\n",
    "        \n",
    "        X=datadict['data'];\n",
    "        Y=datadict['labels'];\n",
    "        \n",
    "        X=X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\");\n",
    "        Y=np.array(Y);\n",
    "        \n",
    "        return X, Y;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "    \"\"\"\n",
    "    unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "    \"\"\"\n",
    "    delta = 1.0 # see notes about delta later in this section\n",
    "    scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "    correct_class_score = scores[y]\n",
    "    D = W.shape[0] # number of classes, e.g. 10\n",
    "    loss_i = 0.0\n",
    "    for j in xrange(D): # iterate over all wrong classes\n",
    "        if j == y:\n",
    "            # skip for the true class to only loop over incorrect classes\n",
    "            continue\n",
    "        # accumulate loss for the i-th example\n",
    "        loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "    return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    \"\"\"\n",
    "    A faster half-vectorized implementation. half-vectorized\n",
    "    refers to the fact that for a single example the implementation contains\n",
    "    no for loops, but there is still one loop over the examples (outside this function)\n",
    "    \"\"\"\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    # compute the margins for all classes in one vector operation\n",
    "    margins = np.maximum(0, scores - scores[y] + delta)\n",
    "    # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "    # to ignore the y-th position and only consider margin on max wrong class\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "    \"\"\" \n",
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "    \"\"\"\n",
    "    # evaluate loss over all examples in X without using any for loops\n",
    "    # left as exercise to reader in the assignment\n",
    "    delta = 1.0\n",
    "    scores = W.dot(X)\n",
    "    num = scores.shape[1]\n",
    "    # remember interger indexing in python, very important(different from MATLAB)\n",
    "    margins = np.maximum(0, scores - scores[y,np.arange(num)] + delta); \n",
    "    margins[y,np.arange(num)] = 0\n",
    "    loss_i = np.sum(margins)/scores.shape[1]\n",
    "    return loss_i\n",
    "def regulizaion(W):\n",
    "    londa = 1.0\n",
    "    rW=np.sum(np.square(W,W))\n",
    "    return rW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xte, Yte = load_CIFAR10('cifar10/') # a magic function we provide\n",
    "# flatten out all images to be one-dimensional\n",
    "Xtr_rows = Xtr.reshape(Xtr.shape[0], 32 * 32 * 3) # Xtr_rows becomes 50000 x 3072\n",
    "Xte_rows = Xte.reshape(Xte.shape[0], 32 * 32 * 3) # Xte_rows becomes 10000 x 3072\n",
    "Xtr_cols = np.concatenate((np.ones((1,50000)),np.transpose(Xtr_rows)))\n",
    "Xte_cols = np.concatenate((np.ones((1,10000)),np.transpose(Xte_rows)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def result(W):\n",
    "    # Assume X_test is [3073 x 10000], Y_test [10000 x 1]\n",
    "    scores = W.dot(Xte_cols) # 10 x 10000, the class scores for all test examples\n",
    "    # find the index with max score in each column (the predicted class)\n",
    "    Yte_predict = np.argmax(scores, axis = 0)\n",
    "    # and calculate accuracy (fraction of predictions that are correct)\n",
    "    print ('accuracy: %f' % ( np.mean((Yte_predict == Yte).astype(np.float32)) ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Strategy 1: Random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attempt 0 the loss was 10.252153, best 10.252153\n",
      "in attempt 1 the loss was 9.945788, best 9.945788\n",
      "in attempt 2 the loss was 9.526387, best 9.526387\n",
      "in attempt 3 the loss was 10.537743, best 9.526387\n",
      "in attempt 4 the loss was 10.298508, best 9.526387\n",
      "in attempt 5 the loss was 9.897546, best 9.526387\n",
      "in attempt 6 the loss was 9.260174, best 9.260174\n",
      "in attempt 7 the loss was 9.562320, best 9.260174\n",
      "in attempt 8 the loss was 9.623974, best 9.260174\n",
      "in attempt 9 the loss was 10.856661, best 9.260174\n",
      "in attempt 10 the loss was 10.260102, best 9.260174\n",
      "in attempt 11 the loss was 10.324253, best 9.260174\n",
      "in attempt 12 the loss was 9.481508, best 9.260174\n",
      "in attempt 13 the loss was 9.650705, best 9.260174\n",
      "in attempt 14 the loss was 9.852696, best 9.260174\n",
      "in attempt 15 the loss was 9.849692, best 9.260174\n",
      "in attempt 16 the loss was 9.844130, best 9.260174\n",
      "in attempt 17 the loss was 10.519446, best 9.260174\n",
      "in attempt 18 the loss was 9.493455, best 9.260174\n",
      "in attempt 19 the loss was 9.796028, best 9.260174\n",
      "in attempt 20 the loss was 9.367023, best 9.260174\n",
      "in attempt 21 the loss was 10.072003, best 9.260174\n",
      "in attempt 22 the loss was 9.324234, best 9.260174\n",
      "in attempt 23 the loss was 9.508518, best 9.260174\n",
      "in attempt 24 the loss was 10.447279, best 9.260174\n",
      "in attempt 25 the loss was 9.721444, best 9.260174\n",
      "in attempt 26 the loss was 10.728164, best 9.260174\n",
      "in attempt 27 the loss was 10.373450, best 9.260174\n",
      "in attempt 28 the loss was 9.677830, best 9.260174\n",
      "in attempt 29 the loss was 10.403454, best 9.260174\n",
      "in attempt 30 the loss was 9.580655, best 9.260174\n",
      "in attempt 31 the loss was 10.704653, best 9.260174\n",
      "in attempt 32 the loss was 9.167818, best 9.167818\n",
      "in attempt 33 the loss was 10.177187, best 9.167818\n",
      "in attempt 34 the loss was 10.297626, best 9.167818\n",
      "in attempt 35 the loss was 9.869914, best 9.167818\n",
      "in attempt 36 the loss was 9.568661, best 9.167818\n",
      "in attempt 37 the loss was 9.938435, best 9.167818\n",
      "in attempt 38 the loss was 10.046742, best 9.167818\n",
      "in attempt 39 the loss was 10.324385, best 9.167818\n",
      "in attempt 40 the loss was 9.835443, best 9.167818\n",
      "in attempt 41 the loss was 10.019365, best 9.167818\n",
      "in attempt 42 the loss was 10.133939, best 9.167818\n",
      "in attempt 43 the loss was 9.411059, best 9.167818\n",
      "in attempt 44 the loss was 9.897624, best 9.167818\n",
      "in attempt 45 the loss was 9.563247, best 9.167818\n",
      "in attempt 46 the loss was 10.100824, best 9.167818\n",
      "in attempt 47 the loss was 10.434476, best 9.167818\n",
      "in attempt 48 the loss was 10.069233, best 9.167818\n",
      "in attempt 49 the loss was 8.687316, best 8.687316\n",
      "in attempt 50 the loss was 9.420654, best 8.687316\n",
      "in attempt 51 the loss was 10.522000, best 8.687316\n",
      "in attempt 52 the loss was 10.426824, best 8.687316\n",
      "in attempt 53 the loss was 10.270240, best 8.687316\n",
      "in attempt 54 the loss was 9.285043, best 8.687316\n",
      "in attempt 55 the loss was 9.676713, best 8.687316\n",
      "in attempt 56 the loss was 10.348336, best 8.687316\n",
      "in attempt 57 the loss was 9.907588, best 8.687316\n",
      "in attempt 58 the loss was 9.550094, best 8.687316\n",
      "in attempt 59 the loss was 9.786773, best 8.687316\n",
      "in attempt 60 the loss was 9.818547, best 8.687316\n",
      "in attempt 61 the loss was 9.575874, best 8.687316\n",
      "in attempt 62 the loss was 9.339076, best 8.687316\n",
      "in attempt 63 the loss was 8.983682, best 8.687316\n",
      "in attempt 64 the loss was 10.079186, best 8.687316\n",
      "in attempt 65 the loss was 9.615582, best 8.687316\n",
      "in attempt 66 the loss was 9.339828, best 8.687316\n",
      "in attempt 67 the loss was 10.657744, best 8.687316\n",
      "in attempt 68 the loss was 9.669274, best 8.687316\n",
      "in attempt 69 the loss was 9.760967, best 8.687316\n",
      "in attempt 70 the loss was 11.818608, best 8.687316\n",
      "in attempt 71 the loss was 9.808859, best 8.687316\n",
      "in attempt 72 the loss was 9.479604, best 8.687316\n",
      "in attempt 73 the loss was 9.824487, best 8.687316\n",
      "in attempt 74 the loss was 9.105034, best 8.687316\n",
      "in attempt 75 the loss was 9.770314, best 8.687316\n",
      "in attempt 76 the loss was 9.856768, best 8.687316\n",
      "in attempt 77 the loss was 10.754197, best 8.687316\n",
      "in attempt 78 the loss was 9.766628, best 8.687316\n",
      "in attempt 79 the loss was 10.965228, best 8.687316\n",
      "in attempt 80 the loss was 9.251384, best 8.687316\n",
      "in attempt 81 the loss was 9.496951, best 8.687316\n",
      "in attempt 82 the loss was 10.017163, best 8.687316\n",
      "in attempt 83 the loss was 10.212156, best 8.687316\n",
      "in attempt 84 the loss was 9.686768, best 8.687316\n",
      "in attempt 85 the loss was 9.533743, best 8.687316\n",
      "in attempt 86 the loss was 9.946118, best 8.687316\n",
      "in attempt 87 the loss was 10.267685, best 8.687316\n",
      "in attempt 88 the loss was 10.653523, best 8.687316\n",
      "in attempt 89 the loss was 9.477665, best 8.687316\n",
      "in attempt 90 the loss was 9.840632, best 8.687316\n",
      "in attempt 91 the loss was 10.866459, best 8.687316\n",
      "in attempt 92 the loss was 9.877225, best 8.687316\n",
      "in attempt 93 the loss was 9.777477, best 8.687316\n",
      "in attempt 94 the loss was 10.105939, best 8.687316\n",
      "in attempt 95 the loss was 11.129772, best 8.687316\n",
      "in attempt 96 the loss was 9.754598, best 8.687316\n",
      "in attempt 97 the loss was 10.452865, best 8.687316\n",
      "in attempt 98 the loss was 9.796734, best 8.687316\n",
      "in attempt 99 the loss was 10.131192, best 8.687316\n",
      "in attempt 100 the loss was 10.593766, best 8.687316\n",
      "in attempt 101 the loss was 10.925581, best 8.687316\n",
      "in attempt 102 the loss was 9.375751, best 8.687316\n",
      "in attempt 103 the loss was 9.615560, best 8.687316\n",
      "in attempt 104 the loss was 9.164198, best 8.687316\n",
      "in attempt 105 the loss was 10.187207, best 8.687316\n",
      "in attempt 106 the loss was 10.682839, best 8.687316\n",
      "in attempt 107 the loss was 10.257694, best 8.687316\n",
      "in attempt 108 the loss was 9.403754, best 8.687316\n",
      "in attempt 109 the loss was 10.034047, best 8.687316\n",
      "in attempt 110 the loss was 9.561629, best 8.687316\n",
      "in attempt 111 the loss was 10.015473, best 8.687316\n",
      "in attempt 112 the loss was 10.324894, best 8.687316\n",
      "in attempt 113 the loss was 9.162995, best 8.687316\n",
      "in attempt 114 the loss was 9.644232, best 8.687316\n",
      "in attempt 115 the loss was 9.502300, best 8.687316\n",
      "in attempt 116 the loss was 9.539014, best 8.687316\n",
      "in attempt 117 the loss was 9.891719, best 8.687316\n",
      "in attempt 118 the loss was 10.641523, best 8.687316\n",
      "in attempt 119 the loss was 10.789028, best 8.687316\n",
      "in attempt 120 the loss was 9.575530, best 8.687316\n",
      "in attempt 121 the loss was 11.452447, best 8.687316\n",
      "in attempt 122 the loss was 10.380571, best 8.687316\n",
      "in attempt 123 the loss was 10.554830, best 8.687316\n",
      "in attempt 124 the loss was 8.797780, best 8.687316\n",
      "in attempt 125 the loss was 8.894732, best 8.687316\n",
      "in attempt 126 the loss was 10.311161, best 8.687316\n",
      "in attempt 127 the loss was 10.360504, best 8.687316\n",
      "in attempt 128 the loss was 10.674089, best 8.687316\n",
      "in attempt 129 the loss was 9.794409, best 8.687316\n",
      "in attempt 130 the loss was 9.884293, best 8.687316\n",
      "in attempt 131 the loss was 10.205147, best 8.687316\n",
      "in attempt 132 the loss was 9.481456, best 8.687316\n",
      "in attempt 133 the loss was 9.914451, best 8.687316\n",
      "in attempt 134 the loss was 10.383394, best 8.687316\n",
      "in attempt 135 the loss was 9.405209, best 8.687316\n",
      "in attempt 136 the loss was 9.030523, best 8.687316\n",
      "in attempt 137 the loss was 10.289551, best 8.687316\n",
      "in attempt 138 the loss was 10.394527, best 8.687316\n",
      "in attempt 139 the loss was 9.208355, best 8.687316\n",
      "in attempt 140 the loss was 10.064866, best 8.687316\n",
      "in attempt 141 the loss was 10.341693, best 8.687316\n",
      "in attempt 142 the loss was 9.819584, best 8.687316\n",
      "in attempt 143 the loss was 10.836498, best 8.687316\n",
      "in attempt 144 the loss was 10.827954, best 8.687316\n",
      "in attempt 145 the loss was 9.271603, best 8.687316\n",
      "in attempt 146 the loss was 9.337675, best 8.687316\n",
      "in attempt 147 the loss was 10.847705, best 8.687316\n",
      "in attempt 148 the loss was 9.760113, best 8.687316\n",
      "in attempt 149 the loss was 11.152159, best 8.687316\n",
      "in attempt 150 the loss was 10.060454, best 8.687316\n",
      "in attempt 151 the loss was 11.127082, best 8.687316\n",
      "in attempt 152 the loss was 9.846631, best 8.687316\n",
      "in attempt 153 the loss was 9.721077, best 8.687316\n",
      "in attempt 154 the loss was 9.840455, best 8.687316\n",
      "in attempt 155 the loss was 9.632222, best 8.687316\n",
      "in attempt 156 the loss was 9.614534, best 8.687316\n",
      "in attempt 157 the loss was 9.242421, best 8.687316\n",
      "in attempt 158 the loss was 10.426368, best 8.687316\n",
      "in attempt 159 the loss was 9.987444, best 8.687316\n",
      "in attempt 160 the loss was 9.926176, best 8.687316\n",
      "in attempt 161 the loss was 9.929369, best 8.687316\n",
      "in attempt 162 the loss was 10.030254, best 8.687316\n",
      "in attempt 163 the loss was 9.605454, best 8.687316\n",
      "in attempt 164 the loss was 10.015512, best 8.687316\n",
      "in attempt 165 the loss was 9.586716, best 8.687316\n",
      "in attempt 166 the loss was 9.689774, best 8.687316\n",
      "in attempt 167 the loss was 9.976798, best 8.687316\n",
      "in attempt 168 the loss was 9.469867, best 8.687316\n",
      "in attempt 169 the loss was 9.199197, best 8.687316\n",
      "in attempt 170 the loss was 10.179192, best 8.687316\n",
      "in attempt 171 the loss was 9.574557, best 8.687316\n",
      "in attempt 172 the loss was 10.164805, best 8.687316\n",
      "in attempt 173 the loss was 10.262612, best 8.687316\n",
      "in attempt 174 the loss was 9.877071, best 8.687316\n",
      "in attempt 175 the loss was 9.612489, best 8.687316\n",
      "in attempt 176 the loss was 10.159360, best 8.687316\n",
      "in attempt 177 the loss was 10.636730, best 8.687316\n",
      "in attempt 178 the loss was 9.946795, best 8.687316\n",
      "in attempt 179 the loss was 10.470907, best 8.687316\n",
      "in attempt 180 the loss was 9.565196, best 8.687316\n",
      "in attempt 181 the loss was 11.113564, best 8.687316\n",
      "in attempt 182 the loss was 9.330731, best 8.687316\n",
      "in attempt 183 the loss was 10.674939, best 8.687316\n",
      "in attempt 184 the loss was 10.978095, best 8.687316\n",
      "in attempt 185 the loss was 9.631930, best 8.687316\n",
      "in attempt 186 the loss was 9.684696, best 8.687316\n",
      "in attempt 187 the loss was 9.290089, best 8.687316\n",
      "in attempt 188 the loss was 9.927938, best 8.687316\n",
      "in attempt 189 the loss was 9.754056, best 8.687316\n",
      "in attempt 190 the loss was 9.749374, best 8.687316\n",
      "in attempt 191 the loss was 9.856098, best 8.687316\n",
      "in attempt 192 the loss was 10.834701, best 8.687316\n",
      "in attempt 193 the loss was 9.695939, best 8.687316\n",
      "in attempt 194 the loss was 11.529604, best 8.687316\n",
      "in attempt 195 the loss was 9.886074, best 8.687316\n",
      "in attempt 196 the loss was 10.479695, best 8.687316\n",
      "in attempt 197 the loss was 10.078859, best 8.687316\n",
      "in attempt 198 the loss was 9.781873, best 8.687316\n",
      "in attempt 199 the loss was 11.536031, best 8.687316\n",
      "in attempt 200 the loss was 9.717724, best 8.687316\n",
      "in attempt 201 the loss was 9.337094, best 8.687316\n",
      "in attempt 202 the loss was 9.361447, best 8.687316\n",
      "in attempt 203 the loss was 9.079621, best 8.687316\n",
      "in attempt 204 the loss was 10.126129, best 8.687316\n",
      "in attempt 205 the loss was 10.627709, best 8.687316\n",
      "in attempt 206 the loss was 10.099831, best 8.687316\n",
      "in attempt 207 the loss was 9.267774, best 8.687316\n",
      "in attempt 208 the loss was 9.715786, best 8.687316\n",
      "in attempt 209 the loss was 9.845477, best 8.687316\n",
      "in attempt 210 the loss was 10.033990, best 8.687316\n",
      "in attempt 211 the loss was 9.402755, best 8.687316\n",
      "in attempt 212 the loss was 10.370524, best 8.687316\n",
      "in attempt 213 the loss was 9.586277, best 8.687316\n",
      "in attempt 214 the loss was 9.457384, best 8.687316\n",
      "in attempt 215 the loss was 9.257494, best 8.687316\n",
      "in attempt 216 the loss was 9.560651, best 8.687316\n",
      "in attempt 217 the loss was 9.025998, best 8.687316\n",
      "in attempt 218 the loss was 9.454135, best 8.687316\n",
      "in attempt 219 the loss was 10.116790, best 8.687316\n",
      "in attempt 220 the loss was 9.609714, best 8.687316\n",
      "in attempt 221 the loss was 11.125002, best 8.687316\n",
      "in attempt 222 the loss was 10.532395, best 8.687316\n",
      "in attempt 223 the loss was 9.386273, best 8.687316\n",
      "in attempt 224 the loss was 9.997045, best 8.687316\n",
      "in attempt 225 the loss was 9.791119, best 8.687316\n",
      "in attempt 226 the loss was 10.328591, best 8.687316\n",
      "in attempt 227 the loss was 9.309390, best 8.687316\n",
      "in attempt 228 the loss was 10.577991, best 8.687316\n",
      "in attempt 229 the loss was 8.831247, best 8.687316\n",
      "in attempt 230 the loss was 11.417677, best 8.687316\n",
      "in attempt 231 the loss was 9.440743, best 8.687316\n",
      "in attempt 232 the loss was 9.416235, best 8.687316\n",
      "in attempt 233 the loss was 9.671856, best 8.687316\n",
      "in attempt 234 the loss was 10.049020, best 8.687316\n",
      "in attempt 235 the loss was 9.093655, best 8.687316\n",
      "in attempt 236 the loss was 9.941949, best 8.687316\n",
      "in attempt 237 the loss was 10.976748, best 8.687316\n",
      "in attempt 238 the loss was 9.350570, best 8.687316\n",
      "in attempt 239 the loss was 10.214305, best 8.687316\n",
      "in attempt 240 the loss was 9.684780, best 8.687316\n",
      "in attempt 241 the loss was 10.441918, best 8.687316\n",
      "in attempt 242 the loss was 9.222808, best 8.687316\n",
      "in attempt 243 the loss was 10.294666, best 8.687316\n",
      "in attempt 244 the loss was 9.825111, best 8.687316\n",
      "in attempt 245 the loss was 10.106066, best 8.687316\n",
      "in attempt 246 the loss was 9.852193, best 8.687316\n",
      "in attempt 247 the loss was 9.728783, best 8.687316\n",
      "in attempt 248 the loss was 10.548309, best 8.687316\n",
      "in attempt 249 the loss was 9.161835, best 8.687316\n",
      "in attempt 250 the loss was 10.303123, best 8.687316\n",
      "in attempt 251 the loss was 10.187708, best 8.687316\n",
      "in attempt 252 the loss was 9.524011, best 8.687316\n",
      "in attempt 253 the loss was 9.792444, best 8.687316\n",
      "in attempt 254 the loss was 9.460217, best 8.687316\n",
      "in attempt 255 the loss was 8.968115, best 8.687316\n",
      "in attempt 256 the loss was 10.120489, best 8.687316\n",
      "in attempt 257 the loss was 9.207303, best 8.687316\n",
      "in attempt 258 the loss was 10.259651, best 8.687316\n",
      "in attempt 259 the loss was 10.160113, best 8.687316\n",
      "in attempt 260 the loss was 9.470390, best 8.687316\n",
      "in attempt 261 the loss was 10.174651, best 8.687316\n",
      "in attempt 262 the loss was 10.858531, best 8.687316\n",
      "in attempt 263 the loss was 9.392199, best 8.687316\n",
      "in attempt 264 the loss was 9.732876, best 8.687316\n",
      "in attempt 265 the loss was 9.094930, best 8.687316\n",
      "in attempt 266 the loss was 10.374955, best 8.687316\n",
      "in attempt 267 the loss was 9.324471, best 8.687316\n",
      "in attempt 268 the loss was 9.360159, best 8.687316\n",
      "in attempt 269 the loss was 10.144363, best 8.687316\n",
      "in attempt 270 the loss was 9.556983, best 8.687316\n",
      "in attempt 271 the loss was 10.314377, best 8.687316\n",
      "in attempt 272 the loss was 9.241318, best 8.687316\n",
      "in attempt 273 the loss was 9.406709, best 8.687316\n",
      "in attempt 274 the loss was 9.318984, best 8.687316\n",
      "in attempt 275 the loss was 9.728786, best 8.687316\n",
      "in attempt 276 the loss was 10.700280, best 8.687316\n",
      "in attempt 277 the loss was 9.278982, best 8.687316\n",
      "in attempt 278 the loss was 9.308421, best 8.687316\n",
      "in attempt 279 the loss was 10.007761, best 8.687316\n",
      "in attempt 280 the loss was 9.777794, best 8.687316\n",
      "in attempt 281 the loss was 9.853673, best 8.687316\n",
      "in attempt 282 the loss was 9.410044, best 8.687316\n",
      "in attempt 283 the loss was 10.166882, best 8.687316\n",
      "in attempt 284 the loss was 9.997729, best 8.687316\n",
      "in attempt 285 the loss was 10.301316, best 8.687316\n",
      "in attempt 286 the loss was 10.895022, best 8.687316\n",
      "in attempt 287 the loss was 9.371537, best 8.687316\n",
      "in attempt 288 the loss was 10.269634, best 8.687316\n",
      "in attempt 289 the loss was 9.949246, best 8.687316\n",
      "in attempt 290 the loss was 10.612886, best 8.687316\n",
      "in attempt 291 the loss was 10.484431, best 8.687316\n",
      "in attempt 292 the loss was 9.332870, best 8.687316\n",
      "in attempt 293 the loss was 9.905305, best 8.687316\n",
      "in attempt 294 the loss was 9.434668, best 8.687316\n",
      "in attempt 295 the loss was 9.737432, best 8.687316\n",
      "in attempt 296 the loss was 9.316718, best 8.687316\n",
      "in attempt 297 the loss was 9.817026, best 8.687316\n",
      "in attempt 298 the loss was 9.554389, best 8.687316\n",
      "in attempt 299 the loss was 10.620733, best 8.687316\n",
      "in attempt 300 the loss was 10.481649, best 8.687316\n",
      "in attempt 301 the loss was 9.380857, best 8.687316\n",
      "in attempt 302 the loss was 10.601329, best 8.687316\n",
      "in attempt 303 the loss was 10.080588, best 8.687316\n",
      "in attempt 304 the loss was 9.257705, best 8.687316\n",
      "in attempt 305 the loss was 10.051264, best 8.687316\n",
      "in attempt 306 the loss was 9.458037, best 8.687316\n",
      "in attempt 307 the loss was 10.326023, best 8.687316\n",
      "in attempt 308 the loss was 9.613835, best 8.687316\n",
      "in attempt 309 the loss was 9.537807, best 8.687316\n",
      "in attempt 310 the loss was 9.082388, best 8.687316\n",
      "in attempt 311 the loss was 9.648989, best 8.687316\n",
      "in attempt 312 the loss was 10.042996, best 8.687316\n",
      "in attempt 313 the loss was 10.860604, best 8.687316\n",
      "in attempt 314 the loss was 9.658186, best 8.687316\n",
      "in attempt 315 the loss was 9.790044, best 8.687316\n",
      "in attempt 316 the loss was 9.362495, best 8.687316\n",
      "in attempt 317 the loss was 10.108722, best 8.687316\n",
      "in attempt 318 the loss was 9.420681, best 8.687316\n",
      "in attempt 319 the loss was 9.502482, best 8.687316\n",
      "in attempt 320 the loss was 11.573194, best 8.687316\n",
      "in attempt 321 the loss was 10.484760, best 8.687316\n",
      "in attempt 322 the loss was 10.344582, best 8.687316\n",
      "in attempt 323 the loss was 10.407672, best 8.687316\n",
      "in attempt 324 the loss was 10.207514, best 8.687316\n",
      "in attempt 325 the loss was 10.816279, best 8.687316\n",
      "in attempt 326 the loss was 9.844698, best 8.687316\n",
      "in attempt 327 the loss was 9.324509, best 8.687316\n",
      "in attempt 328 the loss was 10.861703, best 8.687316\n",
      "in attempt 329 the loss was 9.868433, best 8.687316\n",
      "in attempt 330 the loss was 10.393085, best 8.687316\n",
      "in attempt 331 the loss was 9.305543, best 8.687316\n",
      "in attempt 332 the loss was 9.666911, best 8.687316\n",
      "in attempt 333 the loss was 10.261159, best 8.687316\n",
      "in attempt 334 the loss was 9.435602, best 8.687316\n",
      "in attempt 335 the loss was 10.151508, best 8.687316\n",
      "in attempt 336 the loss was 10.807565, best 8.687316\n",
      "in attempt 337 the loss was 10.805640, best 8.687316\n",
      "in attempt 338 the loss was 10.237267, best 8.687316\n",
      "in attempt 339 the loss was 9.492357, best 8.687316\n",
      "in attempt 340 the loss was 12.167810, best 8.687316\n",
      "in attempt 341 the loss was 9.535949, best 8.687316\n",
      "in attempt 342 the loss was 9.690197, best 8.687316\n",
      "in attempt 343 the loss was 9.220326, best 8.687316\n",
      "in attempt 344 the loss was 9.414353, best 8.687316\n",
      "in attempt 345 the loss was 9.396256, best 8.687316\n",
      "in attempt 346 the loss was 9.542406, best 8.687316\n",
      "in attempt 347 the loss was 9.489615, best 8.687316\n",
      "in attempt 348 the loss was 10.251711, best 8.687316\n",
      "in attempt 349 the loss was 9.998720, best 8.687316\n",
      "in attempt 350 the loss was 10.431371, best 8.687316\n",
      "in attempt 351 the loss was 9.588085, best 8.687316\n",
      "in attempt 352 the loss was 10.841680, best 8.687316\n",
      "in attempt 353 the loss was 9.563972, best 8.687316\n",
      "in attempt 354 the loss was 10.372765, best 8.687316\n",
      "in attempt 355 the loss was 10.038357, best 8.687316\n",
      "in attempt 356 the loss was 9.485738, best 8.687316\n",
      "in attempt 357 the loss was 10.000576, best 8.687316\n",
      "in attempt 358 the loss was 9.635983, best 8.687316\n",
      "in attempt 359 the loss was 10.613445, best 8.687316\n",
      "in attempt 360 the loss was 9.130736, best 8.687316\n",
      "in attempt 361 the loss was 9.490482, best 8.687316\n",
      "in attempt 362 the loss was 10.245880, best 8.687316\n",
      "in attempt 363 the loss was 10.218283, best 8.687316\n",
      "in attempt 364 the loss was 10.228247, best 8.687316\n",
      "in attempt 365 the loss was 9.306889, best 8.687316\n",
      "in attempt 366 the loss was 10.532338, best 8.687316\n",
      "in attempt 367 the loss was 10.545833, best 8.687316\n",
      "in attempt 368 the loss was 10.005717, best 8.687316\n",
      "in attempt 369 the loss was 10.317475, best 8.687316\n",
      "in attempt 370 the loss was 9.438531, best 8.687316\n",
      "in attempt 371 the loss was 10.481420, best 8.687316\n",
      "in attempt 372 the loss was 10.076996, best 8.687316\n",
      "in attempt 373 the loss was 9.678897, best 8.687316\n",
      "in attempt 374 the loss was 9.528512, best 8.687316\n",
      "in attempt 375 the loss was 10.584112, best 8.687316\n",
      "in attempt 376 the loss was 9.873675, best 8.687316\n",
      "in attempt 377 the loss was 9.860122, best 8.687316\n",
      "in attempt 378 the loss was 9.117073, best 8.687316\n",
      "in attempt 379 the loss was 10.433195, best 8.687316\n",
      "in attempt 380 the loss was 9.236638, best 8.687316\n",
      "in attempt 381 the loss was 9.419022, best 8.687316\n",
      "in attempt 382 the loss was 9.417801, best 8.687316\n",
      "in attempt 383 the loss was 10.095870, best 8.687316\n",
      "in attempt 384 the loss was 9.760239, best 8.687316\n",
      "in attempt 385 the loss was 9.626717, best 8.687316\n",
      "in attempt 386 the loss was 9.241313, best 8.687316\n",
      "in attempt 387 the loss was 9.957811, best 8.687316\n",
      "in attempt 388 the loss was 9.926023, best 8.687316\n",
      "in attempt 389 the loss was 9.893734, best 8.687316\n",
      "in attempt 390 the loss was 10.450197, best 8.687316\n",
      "in attempt 391 the loss was 10.127763, best 8.687316\n",
      "in attempt 392 the loss was 9.217153, best 8.687316\n",
      "in attempt 393 the loss was 11.029988, best 8.687316\n",
      "in attempt 394 the loss was 9.652439, best 8.687316\n",
      "in attempt 395 the loss was 10.262647, best 8.687316\n",
      "in attempt 396 the loss was 9.510263, best 8.687316\n",
      "in attempt 397 the loss was 10.020475, best 8.687316\n",
      "in attempt 398 the loss was 9.669066, best 8.687316\n",
      "in attempt 399 the loss was 10.707825, best 8.687316\n",
      "in attempt 400 the loss was 10.339055, best 8.687316\n",
      "in attempt 401 the loss was 9.967147, best 8.687316\n",
      "in attempt 402 the loss was 9.604062, best 8.687316\n",
      "in attempt 403 the loss was 10.835411, best 8.687316\n",
      "in attempt 404 the loss was 9.652927, best 8.687316\n",
      "in attempt 405 the loss was 9.862233, best 8.687316\n",
      "in attempt 406 the loss was 9.855168, best 8.687316\n",
      "in attempt 407 the loss was 10.431075, best 8.687316\n",
      "in attempt 408 the loss was 9.826254, best 8.687316\n",
      "in attempt 409 the loss was 10.206125, best 8.687316\n",
      "in attempt 410 the loss was 9.580612, best 8.687316\n",
      "in attempt 411 the loss was 10.663431, best 8.687316\n",
      "in attempt 412 the loss was 9.788488, best 8.687316\n",
      "in attempt 413 the loss was 10.351430, best 8.687316\n",
      "in attempt 414 the loss was 9.282681, best 8.687316\n",
      "in attempt 415 the loss was 10.533461, best 8.687316\n",
      "in attempt 416 the loss was 9.228902, best 8.687316\n",
      "in attempt 417 the loss was 10.444609, best 8.687316\n",
      "in attempt 418 the loss was 10.809317, best 8.687316\n",
      "in attempt 419 the loss was 10.482592, best 8.687316\n",
      "in attempt 420 the loss was 9.964911, best 8.687316\n",
      "in attempt 421 the loss was 10.327925, best 8.687316\n",
      "in attempt 422 the loss was 9.201417, best 8.687316\n",
      "in attempt 423 the loss was 10.052541, best 8.687316\n",
      "in attempt 424 the loss was 10.061120, best 8.687316\n",
      "in attempt 425 the loss was 9.421176, best 8.687316\n",
      "in attempt 426 the loss was 9.029025, best 8.687316\n",
      "in attempt 427 the loss was 10.100882, best 8.687316\n",
      "in attempt 428 the loss was 9.422994, best 8.687316\n",
      "in attempt 429 the loss was 9.562664, best 8.687316\n",
      "in attempt 430 the loss was 10.724948, best 8.687316\n",
      "in attempt 431 the loss was 9.782223, best 8.687316\n",
      "in attempt 432 the loss was 9.284790, best 8.687316\n",
      "in attempt 433 the loss was 9.582111, best 8.687316\n",
      "in attempt 434 the loss was 9.455061, best 8.687316\n",
      "in attempt 435 the loss was 9.885485, best 8.687316\n",
      "in attempt 436 the loss was 10.186282, best 8.687316\n",
      "in attempt 437 the loss was 9.225859, best 8.687316\n",
      "in attempt 438 the loss was 9.243400, best 8.687316\n",
      "in attempt 439 the loss was 9.111696, best 8.687316\n",
      "in attempt 440 the loss was 10.167308, best 8.687316\n",
      "in attempt 441 the loss was 9.899888, best 8.687316\n",
      "in attempt 442 the loss was 8.762924, best 8.687316\n",
      "in attempt 443 the loss was 10.774333, best 8.687316\n",
      "in attempt 444 the loss was 9.111175, best 8.687316\n",
      "in attempt 445 the loss was 9.650781, best 8.687316\n",
      "in attempt 446 the loss was 10.138778, best 8.687316\n",
      "in attempt 447 the loss was 10.710608, best 8.687316\n",
      "in attempt 448 the loss was 10.606933, best 8.687316\n",
      "in attempt 449 the loss was 10.995716, best 8.687316\n",
      "in attempt 450 the loss was 9.992756, best 8.687316\n",
      "in attempt 451 the loss was 10.623294, best 8.687316\n",
      "in attempt 452 the loss was 9.991311, best 8.687316\n",
      "in attempt 453 the loss was 10.276263, best 8.687316\n",
      "in attempt 454 the loss was 9.967925, best 8.687316\n",
      "in attempt 455 the loss was 9.710973, best 8.687316\n",
      "in attempt 456 the loss was 9.767549, best 8.687316\n",
      "in attempt 457 the loss was 9.094531, best 8.687316\n",
      "in attempt 458 the loss was 10.329878, best 8.687316\n",
      "in attempt 459 the loss was 9.112772, best 8.687316\n",
      "in attempt 460 the loss was 10.189242, best 8.687316\n",
      "in attempt 461 the loss was 9.900508, best 8.687316\n",
      "in attempt 462 the loss was 9.600774, best 8.687316\n",
      "in attempt 463 the loss was 9.934427, best 8.687316\n",
      "in attempt 464 the loss was 10.347426, best 8.687316\n",
      "in attempt 465 the loss was 9.634984, best 8.687316\n",
      "in attempt 466 the loss was 9.147113, best 8.687316\n",
      "in attempt 467 the loss was 10.556486, best 8.687316\n",
      "in attempt 468 the loss was 9.893379, best 8.687316\n",
      "in attempt 469 the loss was 9.960295, best 8.687316\n",
      "in attempt 470 the loss was 9.613570, best 8.687316\n",
      "in attempt 471 the loss was 10.491972, best 8.687316\n",
      "in attempt 472 the loss was 9.839883, best 8.687316\n",
      "in attempt 473 the loss was 10.609626, best 8.687316\n",
      "in attempt 474 the loss was 9.789072, best 8.687316\n",
      "in attempt 475 the loss was 11.377044, best 8.687316\n",
      "in attempt 476 the loss was 9.899973, best 8.687316\n",
      "in attempt 477 the loss was 9.860110, best 8.687316\n",
      "in attempt 478 the loss was 9.535377, best 8.687316\n",
      "in attempt 479 the loss was 10.374048, best 8.687316\n",
      "in attempt 480 the loss was 9.520350, best 8.687316\n",
      "in attempt 481 the loss was 11.117389, best 8.687316\n",
      "in attempt 482 the loss was 9.649867, best 8.687316\n",
      "in attempt 483 the loss was 9.597867, best 8.687316\n",
      "in attempt 484 the loss was 10.134454, best 8.687316\n",
      "in attempt 485 the loss was 9.726587, best 8.687316\n",
      "in attempt 486 the loss was 11.294422, best 8.687316\n",
      "in attempt 487 the loss was 10.275068, best 8.687316\n",
      "in attempt 488 the loss was 10.419130, best 8.687316\n",
      "in attempt 489 the loss was 9.735265, best 8.687316\n",
      "in attempt 490 the loss was 9.910252, best 8.687316\n",
      "in attempt 491 the loss was 9.690418, best 8.687316\n",
      "in attempt 492 the loss was 9.138539, best 8.687316\n",
      "in attempt 493 the loss was 9.383852, best 8.687316\n",
      "in attempt 494 the loss was 9.694461, best 8.687316\n",
      "in attempt 495 the loss was 10.319498, best 8.687316\n",
      "in attempt 496 the loss was 10.107612, best 8.687316\n",
      "in attempt 497 the loss was 9.818331, best 8.687316\n",
      "in attempt 498 the loss was 10.848828, best 8.687316\n",
      "in attempt 499 the loss was 9.491808, best 8.687316\n",
      "in attempt 500 the loss was 9.456961, best 8.687316\n",
      "in attempt 501 the loss was 9.686605, best 8.687316\n",
      "in attempt 502 the loss was 9.809510, best 8.687316\n",
      "in attempt 503 the loss was 9.306131, best 8.687316\n",
      "in attempt 504 the loss was 9.870512, best 8.687316\n",
      "in attempt 505 the loss was 10.480338, best 8.687316\n",
      "in attempt 506 the loss was 9.777005, best 8.687316\n",
      "in attempt 507 the loss was 9.592631, best 8.687316\n",
      "in attempt 508 the loss was 9.702414, best 8.687316\n",
      "in attempt 509 the loss was 9.295624, best 8.687316\n",
      "in attempt 510 the loss was 9.490224, best 8.687316\n",
      "in attempt 511 the loss was 10.238188, best 8.687316\n",
      "in attempt 512 the loss was 10.464156, best 8.687316\n",
      "in attempt 513 the loss was 9.499706, best 8.687316\n",
      "in attempt 514 the loss was 10.455192, best 8.687316\n",
      "in attempt 515 the loss was 10.284286, best 8.687316\n",
      "in attempt 516 the loss was 10.362789, best 8.687316\n",
      "in attempt 517 the loss was 9.998045, best 8.687316\n",
      "in attempt 518 the loss was 9.555356, best 8.687316\n",
      "in attempt 519 the loss was 9.403813, best 8.687316\n",
      "in attempt 520 the loss was 10.560757, best 8.687316\n",
      "in attempt 521 the loss was 9.202806, best 8.687316\n",
      "in attempt 522 the loss was 9.410007, best 8.687316\n",
      "in attempt 523 the loss was 10.299706, best 8.687316\n",
      "in attempt 524 the loss was 9.698163, best 8.687316\n",
      "in attempt 525 the loss was 10.249745, best 8.687316\n",
      "in attempt 526 the loss was 8.877275, best 8.687316\n",
      "in attempt 527 the loss was 9.480706, best 8.687316\n",
      "in attempt 528 the loss was 9.484875, best 8.687316\n",
      "in attempt 529 the loss was 9.466620, best 8.687316\n",
      "in attempt 530 the loss was 9.762032, best 8.687316\n",
      "in attempt 531 the loss was 9.178630, best 8.687316\n",
      "in attempt 532 the loss was 10.610984, best 8.687316\n",
      "in attempt 533 the loss was 9.175632, best 8.687316\n",
      "in attempt 534 the loss was 9.705755, best 8.687316\n",
      "in attempt 535 the loss was 10.193793, best 8.687316\n",
      "in attempt 536 the loss was 10.133557, best 8.687316\n",
      "in attempt 537 the loss was 9.964674, best 8.687316\n",
      "in attempt 538 the loss was 10.025838, best 8.687316\n",
      "in attempt 539 the loss was 10.315656, best 8.687316\n",
      "in attempt 540 the loss was 9.595632, best 8.687316\n",
      "in attempt 541 the loss was 10.252454, best 8.687316\n",
      "in attempt 542 the loss was 10.071493, best 8.687316\n",
      "in attempt 543 the loss was 9.540034, best 8.687316\n",
      "in attempt 544 the loss was 10.671854, best 8.687316\n",
      "in attempt 545 the loss was 9.430651, best 8.687316\n",
      "in attempt 546 the loss was 10.315720, best 8.687316\n",
      "in attempt 547 the loss was 9.099321, best 8.687316\n",
      "in attempt 548 the loss was 9.605685, best 8.687316\n",
      "in attempt 549 the loss was 9.756768, best 8.687316\n",
      "in attempt 550 the loss was 8.993631, best 8.687316\n",
      "in attempt 551 the loss was 10.755736, best 8.687316\n",
      "in attempt 552 the loss was 10.445036, best 8.687316\n",
      "in attempt 553 the loss was 9.810327, best 8.687316\n",
      "in attempt 554 the loss was 9.767392, best 8.687316\n",
      "in attempt 555 the loss was 10.308342, best 8.687316\n",
      "in attempt 556 the loss was 10.673447, best 8.687316\n",
      "in attempt 557 the loss was 9.274324, best 8.687316\n",
      "in attempt 558 the loss was 10.309082, best 8.687316\n",
      "in attempt 559 the loss was 10.262865, best 8.687316\n",
      "in attempt 560 the loss was 11.270764, best 8.687316\n",
      "in attempt 561 the loss was 10.677085, best 8.687316\n",
      "in attempt 562 the loss was 10.861763, best 8.687316\n",
      "in attempt 563 the loss was 10.403659, best 8.687316\n",
      "in attempt 564 the loss was 9.784637, best 8.687316\n",
      "in attempt 565 the loss was 10.623251, best 8.687316\n",
      "in attempt 566 the loss was 10.916737, best 8.687316\n",
      "in attempt 567 the loss was 10.680274, best 8.687316\n",
      "in attempt 568 the loss was 9.501525, best 8.687316\n",
      "in attempt 569 the loss was 9.837854, best 8.687316\n",
      "in attempt 570 the loss was 9.240054, best 8.687316\n",
      "in attempt 571 the loss was 9.697668, best 8.687316\n",
      "in attempt 572 the loss was 9.307736, best 8.687316\n",
      "in attempt 573 the loss was 10.112782, best 8.687316\n",
      "in attempt 574 the loss was 10.797311, best 8.687316\n",
      "in attempt 575 the loss was 9.686860, best 8.687316\n",
      "in attempt 576 the loss was 9.745449, best 8.687316\n",
      "in attempt 577 the loss was 9.338225, best 8.687316\n",
      "in attempt 578 the loss was 8.739046, best 8.687316\n",
      "in attempt 579 the loss was 9.561977, best 8.687316\n",
      "in attempt 580 the loss was 9.710337, best 8.687316\n",
      "in attempt 581 the loss was 10.556292, best 8.687316\n",
      "in attempt 582 the loss was 10.218086, best 8.687316\n",
      "in attempt 583 the loss was 9.429960, best 8.687316\n",
      "in attempt 584 the loss was 9.653562, best 8.687316\n",
      "in attempt 585 the loss was 10.107257, best 8.687316\n",
      "in attempt 586 the loss was 9.666414, best 8.687316\n",
      "in attempt 587 the loss was 9.761610, best 8.687316\n",
      "in attempt 588 the loss was 10.003577, best 8.687316\n",
      "in attempt 589 the loss was 9.791931, best 8.687316\n",
      "in attempt 590 the loss was 9.444822, best 8.687316\n",
      "in attempt 591 the loss was 10.256296, best 8.687316\n",
      "in attempt 592 the loss was 9.300207, best 8.687316\n",
      "in attempt 593 the loss was 9.549111, best 8.687316\n",
      "in attempt 594 the loss was 10.056256, best 8.687316\n",
      "in attempt 595 the loss was 10.230985, best 8.687316\n",
      "in attempt 596 the loss was 9.539376, best 8.687316\n",
      "in attempt 597 the loss was 10.020811, best 8.687316\n",
      "in attempt 598 the loss was 9.313116, best 8.687316\n",
      "in attempt 599 the loss was 10.578889, best 8.687316\n",
      "in attempt 600 the loss was 10.169452, best 8.687316\n",
      "in attempt 601 the loss was 9.490704, best 8.687316\n",
      "in attempt 602 the loss was 9.899823, best 8.687316\n",
      "in attempt 603 the loss was 9.659413, best 8.687316\n",
      "in attempt 604 the loss was 10.108599, best 8.687316\n",
      "in attempt 605 the loss was 11.279230, best 8.687316\n",
      "in attempt 606 the loss was 9.709259, best 8.687316\n",
      "in attempt 607 the loss was 9.659618, best 8.687316\n",
      "in attempt 608 the loss was 10.198258, best 8.687316\n",
      "in attempt 609 the loss was 9.389646, best 8.687316\n",
      "in attempt 610 the loss was 9.466782, best 8.687316\n",
      "in attempt 611 the loss was 10.113876, best 8.687316\n",
      "in attempt 612 the loss was 10.922439, best 8.687316\n",
      "in attempt 613 the loss was 9.935826, best 8.687316\n",
      "in attempt 614 the loss was 9.303823, best 8.687316\n",
      "in attempt 615 the loss was 10.282845, best 8.687316\n",
      "in attempt 616 the loss was 9.457695, best 8.687316\n",
      "in attempt 617 the loss was 10.393583, best 8.687316\n",
      "in attempt 618 the loss was 9.827568, best 8.687316\n",
      "in attempt 619 the loss was 9.892856, best 8.687316\n",
      "in attempt 620 the loss was 10.170194, best 8.687316\n",
      "in attempt 621 the loss was 9.772949, best 8.687316\n",
      "in attempt 622 the loss was 10.273801, best 8.687316\n",
      "in attempt 623 the loss was 9.543780, best 8.687316\n",
      "in attempt 624 the loss was 9.865563, best 8.687316\n",
      "in attempt 625 the loss was 9.673728, best 8.687316\n",
      "in attempt 626 the loss was 9.721300, best 8.687316\n",
      "in attempt 627 the loss was 9.289335, best 8.687316\n",
      "in attempt 628 the loss was 10.537396, best 8.687316\n",
      "in attempt 629 the loss was 9.559945, best 8.687316\n",
      "in attempt 630 the loss was 10.106777, best 8.687316\n",
      "in attempt 631 the loss was 10.554339, best 8.687316\n",
      "in attempt 632 the loss was 10.303232, best 8.687316\n",
      "in attempt 633 the loss was 10.108758, best 8.687316\n",
      "in attempt 634 the loss was 9.453977, best 8.687316\n",
      "in attempt 635 the loss was 9.843449, best 8.687316\n",
      "in attempt 636 the loss was 9.110961, best 8.687316\n",
      "in attempt 637 the loss was 10.140619, best 8.687316\n",
      "in attempt 638 the loss was 10.020562, best 8.687316\n",
      "in attempt 639 the loss was 10.701501, best 8.687316\n",
      "in attempt 640 the loss was 9.036571, best 8.687316\n",
      "in attempt 641 the loss was 10.467217, best 8.687316\n",
      "in attempt 642 the loss was 10.146193, best 8.687316\n",
      "in attempt 643 the loss was 11.527547, best 8.687316\n",
      "in attempt 644 the loss was 9.928065, best 8.687316\n",
      "in attempt 645 the loss was 9.608503, best 8.687316\n",
      "in attempt 646 the loss was 9.995756, best 8.687316\n",
      "in attempt 647 the loss was 10.875723, best 8.687316\n",
      "in attempt 648 the loss was 10.322107, best 8.687316\n",
      "in attempt 649 the loss was 10.031669, best 8.687316\n",
      "in attempt 650 the loss was 9.657469, best 8.687316\n",
      "in attempt 651 the loss was 9.759012, best 8.687316\n",
      "in attempt 652 the loss was 9.841239, best 8.687316\n",
      "in attempt 653 the loss was 10.342813, best 8.687316\n",
      "in attempt 654 the loss was 10.407374, best 8.687316\n",
      "in attempt 655 the loss was 9.857477, best 8.687316\n",
      "in attempt 656 the loss was 9.745638, best 8.687316\n",
      "in attempt 657 the loss was 9.697135, best 8.687316\n",
      "in attempt 658 the loss was 9.141200, best 8.687316\n",
      "in attempt 659 the loss was 9.594212, best 8.687316\n",
      "in attempt 660 the loss was 9.660520, best 8.687316\n",
      "in attempt 661 the loss was 10.912550, best 8.687316\n",
      "in attempt 662 the loss was 9.628625, best 8.687316\n",
      "in attempt 663 the loss was 9.684094, best 8.687316\n",
      "in attempt 664 the loss was 11.127186, best 8.687316\n",
      "in attempt 665 the loss was 10.602257, best 8.687316\n",
      "in attempt 666 the loss was 10.075886, best 8.687316\n",
      "in attempt 667 the loss was 9.056432, best 8.687316\n",
      "in attempt 668 the loss was 10.311862, best 8.687316\n",
      "in attempt 669 the loss was 9.902144, best 8.687316\n",
      "in attempt 670 the loss was 10.136220, best 8.687316\n",
      "in attempt 671 the loss was 9.689444, best 8.687316\n",
      "in attempt 672 the loss was 9.929485, best 8.687316\n",
      "in attempt 673 the loss was 10.009071, best 8.687316\n",
      "in attempt 674 the loss was 10.227919, best 8.687316\n",
      "in attempt 675 the loss was 9.701638, best 8.687316\n",
      "in attempt 676 the loss was 9.078443, best 8.687316\n",
      "in attempt 677 the loss was 9.855462, best 8.687316\n",
      "in attempt 678 the loss was 9.517940, best 8.687316\n",
      "in attempt 679 the loss was 9.869133, best 8.687316\n",
      "in attempt 680 the loss was 9.742038, best 8.687316\n",
      "in attempt 681 the loss was 9.784737, best 8.687316\n",
      "in attempt 682 the loss was 9.805244, best 8.687316\n",
      "in attempt 683 the loss was 10.135946, best 8.687316\n",
      "in attempt 684 the loss was 10.378236, best 8.687316\n",
      "in attempt 685 the loss was 10.416288, best 8.687316\n",
      "in attempt 686 the loss was 10.097223, best 8.687316\n",
      "in attempt 687 the loss was 10.406574, best 8.687316\n",
      "in attempt 688 the loss was 9.650894, best 8.687316\n",
      "in attempt 689 the loss was 9.467943, best 8.687316\n",
      "in attempt 690 the loss was 9.356321, best 8.687316\n",
      "in attempt 691 the loss was 9.954227, best 8.687316\n",
      "in attempt 692 the loss was 9.351097, best 8.687316\n",
      "in attempt 693 the loss was 9.674504, best 8.687316\n",
      "in attempt 694 the loss was 9.937491, best 8.687316\n",
      "in attempt 695 the loss was 9.594056, best 8.687316\n",
      "in attempt 696 the loss was 10.193861, best 8.687316\n",
      "in attempt 697 the loss was 9.959009, best 8.687316\n",
      "in attempt 698 the loss was 10.243948, best 8.687316\n",
      "in attempt 699 the loss was 9.905715, best 8.687316\n",
      "in attempt 700 the loss was 9.621513, best 8.687316\n",
      "in attempt 701 the loss was 9.576241, best 8.687316\n",
      "in attempt 702 the loss was 10.712055, best 8.687316\n",
      "in attempt 703 the loss was 9.600590, best 8.687316\n",
      "in attempt 704 the loss was 9.886115, best 8.687316\n",
      "in attempt 705 the loss was 9.172176, best 8.687316\n",
      "in attempt 706 the loss was 9.022243, best 8.687316\n",
      "in attempt 707 the loss was 10.262512, best 8.687316\n",
      "in attempt 708 the loss was 10.953801, best 8.687316\n",
      "in attempt 709 the loss was 10.901654, best 8.687316\n",
      "in attempt 710 the loss was 9.577788, best 8.687316\n",
      "in attempt 711 the loss was 9.230775, best 8.687316\n",
      "in attempt 712 the loss was 9.897091, best 8.687316\n",
      "in attempt 713 the loss was 9.762947, best 8.687316\n",
      "in attempt 714 the loss was 10.036776, best 8.687316\n",
      "in attempt 715 the loss was 10.541807, best 8.687316\n",
      "in attempt 716 the loss was 10.813309, best 8.687316\n",
      "in attempt 717 the loss was 10.602004, best 8.687316\n",
      "in attempt 718 the loss was 10.176996, best 8.687316\n",
      "in attempt 719 the loss was 11.297195, best 8.687316\n",
      "in attempt 720 the loss was 10.034108, best 8.687316\n",
      "in attempt 721 the loss was 11.020335, best 8.687316\n",
      "in attempt 722 the loss was 9.115907, best 8.687316\n",
      "in attempt 723 the loss was 11.009515, best 8.687316\n",
      "in attempt 724 the loss was 9.652862, best 8.687316\n",
      "in attempt 725 the loss was 10.702858, best 8.687316\n",
      "in attempt 726 the loss was 9.541032, best 8.687316\n",
      "in attempt 727 the loss was 9.136295, best 8.687316\n",
      "in attempt 728 the loss was 9.428032, best 8.687316\n",
      "in attempt 729 the loss was 10.121505, best 8.687316\n",
      "in attempt 730 the loss was 9.537592, best 8.687316\n",
      "in attempt 731 the loss was 9.434685, best 8.687316\n",
      "in attempt 732 the loss was 9.954069, best 8.687316\n",
      "in attempt 733 the loss was 9.711830, best 8.687316\n",
      "in attempt 734 the loss was 10.083895, best 8.687316\n",
      "in attempt 735 the loss was 9.477719, best 8.687316\n",
      "in attempt 736 the loss was 10.360569, best 8.687316\n",
      "in attempt 737 the loss was 10.006943, best 8.687316\n",
      "in attempt 738 the loss was 10.018333, best 8.687316\n",
      "in attempt 739 the loss was 10.462863, best 8.687316\n",
      "in attempt 740 the loss was 9.916372, best 8.687316\n",
      "in attempt 741 the loss was 10.681512, best 8.687316\n",
      "in attempt 742 the loss was 9.870645, best 8.687316\n",
      "in attempt 743 the loss was 9.900203, best 8.687316\n",
      "in attempt 744 the loss was 9.964597, best 8.687316\n",
      "in attempt 745 the loss was 10.133551, best 8.687316\n",
      "in attempt 746 the loss was 10.169203, best 8.687316\n",
      "in attempt 747 the loss was 10.452268, best 8.687316\n",
      "in attempt 748 the loss was 10.161849, best 8.687316\n",
      "in attempt 749 the loss was 9.128033, best 8.687316\n",
      "in attempt 750 the loss was 9.665883, best 8.687316\n",
      "in attempt 751 the loss was 10.259291, best 8.687316\n",
      "in attempt 752 the loss was 10.033207, best 8.687316\n",
      "in attempt 753 the loss was 9.523623, best 8.687316\n",
      "in attempt 754 the loss was 9.190334, best 8.687316\n",
      "in attempt 755 the loss was 9.511607, best 8.687316\n",
      "in attempt 756 the loss was 10.000740, best 8.687316\n",
      "in attempt 757 the loss was 10.114060, best 8.687316\n",
      "in attempt 758 the loss was 10.483370, best 8.687316\n",
      "in attempt 759 the loss was 9.589791, best 8.687316\n",
      "in attempt 760 the loss was 9.857272, best 8.687316\n",
      "in attempt 761 the loss was 9.350775, best 8.687316\n",
      "in attempt 762 the loss was 10.181346, best 8.687316\n",
      "in attempt 763 the loss was 9.789795, best 8.687316\n",
      "in attempt 764 the loss was 11.155429, best 8.687316\n",
      "in attempt 765 the loss was 10.062855, best 8.687316\n",
      "in attempt 766 the loss was 9.926929, best 8.687316\n",
      "in attempt 767 the loss was 10.325367, best 8.687316\n",
      "in attempt 768 the loss was 10.046120, best 8.687316\n",
      "in attempt 769 the loss was 10.384668, best 8.687316\n",
      "in attempt 770 the loss was 11.004991, best 8.687316\n",
      "in attempt 771 the loss was 9.288382, best 8.687316\n",
      "in attempt 772 the loss was 10.349994, best 8.687316\n",
      "in attempt 773 the loss was 10.465008, best 8.687316\n",
      "in attempt 774 the loss was 10.816626, best 8.687316\n",
      "in attempt 775 the loss was 9.824757, best 8.687316\n",
      "in attempt 776 the loss was 9.905137, best 8.687316\n",
      "in attempt 777 the loss was 9.987037, best 8.687316\n",
      "in attempt 778 the loss was 10.462094, best 8.687316\n",
      "in attempt 779 the loss was 9.342124, best 8.687316\n",
      "in attempt 780 the loss was 9.184922, best 8.687316\n",
      "in attempt 781 the loss was 10.811576, best 8.687316\n",
      "in attempt 782 the loss was 9.677661, best 8.687316\n",
      "in attempt 783 the loss was 9.730299, best 8.687316\n",
      "in attempt 784 the loss was 9.830059, best 8.687316\n",
      "in attempt 785 the loss was 10.253644, best 8.687316\n",
      "in attempt 786 the loss was 9.669724, best 8.687316\n",
      "in attempt 787 the loss was 9.762787, best 8.687316\n",
      "in attempt 788 the loss was 8.801623, best 8.687316\n",
      "in attempt 789 the loss was 10.799130, best 8.687316\n",
      "in attempt 790 the loss was 10.520435, best 8.687316\n",
      "in attempt 791 the loss was 9.092030, best 8.687316\n",
      "in attempt 792 the loss was 10.494841, best 8.687316\n",
      "in attempt 793 the loss was 9.784980, best 8.687316\n",
      "in attempt 794 the loss was 10.112317, best 8.687316\n",
      "in attempt 795 the loss was 10.080399, best 8.687316\n",
      "in attempt 796 the loss was 10.236174, best 8.687316\n",
      "in attempt 797 the loss was 9.617808, best 8.687316\n",
      "in attempt 798 the loss was 9.944784, best 8.687316\n",
      "in attempt 799 the loss was 9.632583, best 8.687316\n",
      "in attempt 800 the loss was 10.018350, best 8.687316\n",
      "in attempt 801 the loss was 10.014548, best 8.687316\n",
      "in attempt 802 the loss was 10.545307, best 8.687316\n",
      "in attempt 803 the loss was 10.052243, best 8.687316\n",
      "in attempt 804 the loss was 8.851721, best 8.687316\n",
      "in attempt 805 the loss was 10.265121, best 8.687316\n",
      "in attempt 806 the loss was 8.802527, best 8.687316\n",
      "in attempt 807 the loss was 9.927260, best 8.687316\n",
      "in attempt 808 the loss was 9.617287, best 8.687316\n",
      "in attempt 809 the loss was 9.798468, best 8.687316\n",
      "in attempt 810 the loss was 10.000885, best 8.687316\n",
      "in attempt 811 the loss was 10.485977, best 8.687316\n",
      "in attempt 812 the loss was 10.150147, best 8.687316\n",
      "in attempt 813 the loss was 9.679218, best 8.687316\n",
      "in attempt 814 the loss was 9.369264, best 8.687316\n",
      "in attempt 815 the loss was 10.332829, best 8.687316\n",
      "in attempt 816 the loss was 10.306671, best 8.687316\n",
      "in attempt 817 the loss was 9.470137, best 8.687316\n",
      "in attempt 818 the loss was 9.276160, best 8.687316\n",
      "in attempt 819 the loss was 9.833960, best 8.687316\n",
      "in attempt 820 the loss was 9.544655, best 8.687316\n",
      "in attempt 821 the loss was 10.457014, best 8.687316\n",
      "in attempt 822 the loss was 9.732632, best 8.687316\n",
      "in attempt 823 the loss was 10.339740, best 8.687316\n",
      "in attempt 824 the loss was 10.859725, best 8.687316\n",
      "in attempt 825 the loss was 10.148269, best 8.687316\n",
      "in attempt 826 the loss was 9.584507, best 8.687316\n",
      "in attempt 827 the loss was 9.903570, best 8.687316\n",
      "in attempt 828 the loss was 9.064283, best 8.687316\n",
      "in attempt 829 the loss was 10.872991, best 8.687316\n",
      "in attempt 830 the loss was 9.237222, best 8.687316\n",
      "in attempt 831 the loss was 9.805224, best 8.687316\n",
      "in attempt 832 the loss was 9.612829, best 8.687316\n",
      "in attempt 833 the loss was 10.373767, best 8.687316\n",
      "in attempt 834 the loss was 9.340372, best 8.687316\n",
      "in attempt 835 the loss was 8.997606, best 8.687316\n",
      "in attempt 836 the loss was 9.218757, best 8.687316\n",
      "in attempt 837 the loss was 9.066333, best 8.687316\n",
      "in attempt 838 the loss was 9.806994, best 8.687316\n",
      "in attempt 839 the loss was 9.978719, best 8.687316\n",
      "in attempt 840 the loss was 9.370660, best 8.687316\n",
      "in attempt 841 the loss was 9.015210, best 8.687316\n",
      "in attempt 842 the loss was 10.190831, best 8.687316\n",
      "in attempt 843 the loss was 9.446251, best 8.687316\n",
      "in attempt 844 the loss was 10.252195, best 8.687316\n",
      "in attempt 845 the loss was 10.383369, best 8.687316\n",
      "in attempt 846 the loss was 9.912806, best 8.687316\n",
      "in attempt 847 the loss was 9.856641, best 8.687316\n",
      "in attempt 848 the loss was 9.952696, best 8.687316\n",
      "in attempt 849 the loss was 9.629758, best 8.687316\n",
      "in attempt 850 the loss was 10.132721, best 8.687316\n",
      "in attempt 851 the loss was 9.975783, best 8.687316\n",
      "in attempt 852 the loss was 11.012390, best 8.687316\n",
      "in attempt 853 the loss was 10.822760, best 8.687316\n",
      "in attempt 854 the loss was 9.440253, best 8.687316\n",
      "in attempt 855 the loss was 9.765792, best 8.687316\n",
      "in attempt 856 the loss was 10.796228, best 8.687316\n",
      "in attempt 857 the loss was 9.949799, best 8.687316\n",
      "in attempt 858 the loss was 9.506106, best 8.687316\n",
      "in attempt 859 the loss was 11.012386, best 8.687316\n",
      "in attempt 860 the loss was 9.897625, best 8.687316\n",
      "in attempt 861 the loss was 9.691233, best 8.687316\n",
      "in attempt 862 the loss was 9.233953, best 8.687316\n",
      "in attempt 863 the loss was 9.812630, best 8.687316\n",
      "in attempt 864 the loss was 11.578402, best 8.687316\n",
      "in attempt 865 the loss was 10.664490, best 8.687316\n",
      "in attempt 866 the loss was 9.469368, best 8.687316\n",
      "in attempt 867 the loss was 9.921492, best 8.687316\n",
      "in attempt 868 the loss was 10.766820, best 8.687316\n",
      "in attempt 869 the loss was 9.586425, best 8.687316\n",
      "in attempt 870 the loss was 10.150911, best 8.687316\n",
      "in attempt 871 the loss was 9.737620, best 8.687316\n",
      "in attempt 872 the loss was 11.123892, best 8.687316\n",
      "in attempt 873 the loss was 10.036860, best 8.687316\n",
      "in attempt 874 the loss was 9.557854, best 8.687316\n",
      "in attempt 875 the loss was 10.917208, best 8.687316\n",
      "in attempt 876 the loss was 11.005391, best 8.687316\n",
      "in attempt 877 the loss was 9.598914, best 8.687316\n",
      "in attempt 878 the loss was 10.107123, best 8.687316\n",
      "in attempt 879 the loss was 10.394135, best 8.687316\n",
      "in attempt 880 the loss was 10.286957, best 8.687316\n",
      "in attempt 881 the loss was 9.833534, best 8.687316\n",
      "in attempt 882 the loss was 9.735586, best 8.687316\n",
      "in attempt 883 the loss was 9.717833, best 8.687316\n",
      "in attempt 884 the loss was 9.767521, best 8.687316\n",
      "in attempt 885 the loss was 10.647674, best 8.687316\n",
      "in attempt 886 the loss was 9.491939, best 8.687316\n",
      "in attempt 887 the loss was 9.421923, best 8.687316\n",
      "in attempt 888 the loss was 9.820650, best 8.687316\n",
      "in attempt 889 the loss was 10.582822, best 8.687316\n",
      "in attempt 890 the loss was 9.380780, best 8.687316\n",
      "in attempt 891 the loss was 9.273838, best 8.687316\n",
      "in attempt 892 the loss was 9.122737, best 8.687316\n",
      "in attempt 893 the loss was 9.798263, best 8.687316\n",
      "in attempt 894 the loss was 9.804993, best 8.687316\n",
      "in attempt 895 the loss was 10.356200, best 8.687316\n",
      "in attempt 896 the loss was 9.628451, best 8.687316\n",
      "in attempt 897 the loss was 9.690656, best 8.687316\n",
      "in attempt 898 the loss was 9.615975, best 8.687316\n",
      "in attempt 899 the loss was 10.072476, best 8.687316\n",
      "in attempt 900 the loss was 10.284522, best 8.687316\n",
      "in attempt 901 the loss was 9.710790, best 8.687316\n",
      "in attempt 902 the loss was 10.334918, best 8.687316\n",
      "in attempt 903 the loss was 9.767617, best 8.687316\n",
      "in attempt 904 the loss was 9.628768, best 8.687316\n",
      "in attempt 905 the loss was 9.654226, best 8.687316\n",
      "in attempt 906 the loss was 10.295624, best 8.687316\n",
      "in attempt 907 the loss was 10.291203, best 8.687316\n",
      "in attempt 908 the loss was 10.267258, best 8.687316\n",
      "in attempt 909 the loss was 9.679736, best 8.687316\n",
      "in attempt 910 the loss was 10.105562, best 8.687316\n",
      "in attempt 911 the loss was 10.580714, best 8.687316\n",
      "in attempt 912 the loss was 10.210510, best 8.687316\n",
      "in attempt 913 the loss was 9.205569, best 8.687316\n",
      "in attempt 914 the loss was 9.510663, best 8.687316\n",
      "in attempt 915 the loss was 9.679020, best 8.687316\n",
      "in attempt 916 the loss was 9.867415, best 8.687316\n",
      "in attempt 917 the loss was 9.199164, best 8.687316\n",
      "in attempt 918 the loss was 10.566621, best 8.687316\n",
      "in attempt 919 the loss was 10.108308, best 8.687316\n",
      "in attempt 920 the loss was 10.047733, best 8.687316\n",
      "in attempt 921 the loss was 9.741274, best 8.687316\n",
      "in attempt 922 the loss was 9.037721, best 8.687316\n",
      "in attempt 923 the loss was 11.252692, best 8.687316\n",
      "in attempt 924 the loss was 10.253722, best 8.687316\n",
      "in attempt 925 the loss was 10.763211, best 8.687316\n",
      "in attempt 926 the loss was 9.856084, best 8.687316\n",
      "in attempt 927 the loss was 9.591788, best 8.687316\n",
      "in attempt 928 the loss was 9.468672, best 8.687316\n",
      "in attempt 929 the loss was 9.702504, best 8.687316\n",
      "in attempt 930 the loss was 10.014329, best 8.687316\n",
      "in attempt 931 the loss was 9.891745, best 8.687316\n",
      "in attempt 932 the loss was 9.861550, best 8.687316\n",
      "in attempt 933 the loss was 10.034236, best 8.687316\n",
      "in attempt 934 the loss was 10.185951, best 8.687316\n",
      "in attempt 935 the loss was 9.324681, best 8.687316\n",
      "in attempt 936 the loss was 10.418228, best 8.687316\n",
      "in attempt 937 the loss was 10.000683, best 8.687316\n",
      "in attempt 938 the loss was 10.147362, best 8.687316\n",
      "in attempt 939 the loss was 10.411007, best 8.687316\n",
      "in attempt 940 the loss was 9.962697, best 8.687316\n",
      "in attempt 941 the loss was 9.428482, best 8.687316\n",
      "in attempt 942 the loss was 11.070017, best 8.687316\n",
      "in attempt 943 the loss was 10.153723, best 8.687316\n",
      "in attempt 944 the loss was 10.924327, best 8.687316\n",
      "in attempt 945 the loss was 9.735772, best 8.687316\n",
      "in attempt 946 the loss was 9.851400, best 8.687316\n",
      "in attempt 947 the loss was 10.728116, best 8.687316\n",
      "in attempt 948 the loss was 9.845481, best 8.687316\n",
      "in attempt 949 the loss was 11.022394, best 8.687316\n",
      "in attempt 950 the loss was 9.976428, best 8.687316\n",
      "in attempt 951 the loss was 9.448172, best 8.687316\n",
      "in attempt 952 the loss was 10.444694, best 8.687316\n",
      "in attempt 953 the loss was 9.256069, best 8.687316\n",
      "in attempt 954 the loss was 9.354363, best 8.687316\n",
      "in attempt 955 the loss was 9.391240, best 8.687316\n",
      "in attempt 956 the loss was 10.049657, best 8.687316\n",
      "in attempt 957 the loss was 9.795385, best 8.687316\n",
      "in attempt 958 the loss was 9.437758, best 8.687316\n",
      "in attempt 959 the loss was 10.130973, best 8.687316\n",
      "in attempt 960 the loss was 10.153353, best 8.687316\n",
      "in attempt 961 the loss was 9.519785, best 8.687316\n",
      "in attempt 962 the loss was 10.889278, best 8.687316\n",
      "in attempt 963 the loss was 9.811598, best 8.687316\n",
      "in attempt 964 the loss was 11.469433, best 8.687316\n",
      "in attempt 965 the loss was 9.495703, best 8.687316\n",
      "in attempt 966 the loss was 10.709988, best 8.687316\n",
      "in attempt 967 the loss was 9.901628, best 8.687316\n",
      "in attempt 968 the loss was 9.272600, best 8.687316\n",
      "in attempt 969 the loss was 9.333909, best 8.687316\n",
      "in attempt 970 the loss was 10.177593, best 8.687316\n",
      "in attempt 971 the loss was 10.413500, best 8.687316\n",
      "in attempt 972 the loss was 10.350188, best 8.687316\n",
      "in attempt 973 the loss was 10.834072, best 8.687316\n",
      "in attempt 974 the loss was 10.228854, best 8.687316\n",
      "in attempt 975 the loss was 9.207780, best 8.687316\n",
      "in attempt 976 the loss was 10.306690, best 8.687316\n",
      "in attempt 977 the loss was 9.688855, best 8.687316\n",
      "in attempt 978 the loss was 10.446851, best 8.687316\n",
      "in attempt 979 the loss was 9.611659, best 8.687316\n",
      "in attempt 980 the loss was 9.585047, best 8.687316\n",
      "in attempt 981 the loss was 9.279121, best 8.687316\n",
      "in attempt 982 the loss was 9.908287, best 8.687316\n",
      "in attempt 983 the loss was 9.394032, best 8.687316\n",
      "in attempt 984 the loss was 10.417509, best 8.687316\n",
      "in attempt 985 the loss was 10.011832, best 8.687316\n",
      "in attempt 986 the loss was 10.134606, best 8.687316\n",
      "in attempt 987 the loss was 10.663266, best 8.687316\n",
      "in attempt 988 the loss was 9.475425, best 8.687316\n",
      "in attempt 989 the loss was 10.857837, best 8.687316\n",
      "in attempt 990 the loss was 9.760378, best 8.687316\n",
      "in attempt 991 the loss was 10.760252, best 8.687316\n",
      "in attempt 992 the loss was 10.011891, best 8.687316\n",
      "in attempt 993 the loss was 9.969177, best 8.687316\n",
      "in attempt 994 the loss was 9.704847, best 8.687316\n",
      "in attempt 995 the loss was 9.269084, best 8.687316\n",
      "in attempt 996 the loss was 9.978152, best 8.687316\n",
      "in attempt 997 the loss was 10.927450, best 8.687316\n",
      "in attempt 998 the loss was 9.690012, best 8.687316\n",
      "in attempt 999 the loss was 9.095402, best 8.687316\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12429999999999999"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assume X_train is the data where each column is an example (e.g. 3073 x 50,000)\n",
    "# assume Y_train are the labels (e.g. 1D array of 50,000)\n",
    "# assume the function L evaluates the loss function\n",
    "\n",
    "bestloss = float(\"inf\") # Python assigns the highest possible float value\n",
    "for num in range(1000):\n",
    "    W = np.random.randn(10, 3073) * 0.0001 # generate random parameters\n",
    "    loss = L(Xtr_cols, Ytr, W) # get the loss over the entire training set\n",
    "    if loss < bestloss: # keep track of the best solution\n",
    "        bestloss = loss\n",
    "        Wbest = W\n",
    "    print ('in attempt %d the loss was %f, best %f' % (num, loss, bestloss))\n",
    "\n",
    "result(Wbest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Strategy 2: Random Local Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss is 45.345350\n",
      "iter 1 loss is 44.377905\n",
      "iter 2 loss is 44.377905\n",
      "iter 3 loss is 44.048173\n",
      "iter 4 loss is 42.654332\n",
      "iter 5 loss is 42.654332\n",
      "iter 6 loss is 42.654332\n",
      "iter 7 loss is 42.654332\n",
      "iter 8 loss is 42.348042\n",
      "iter 9 loss is 42.348042\n",
      "iter 10 loss is 41.412049\n",
      "iter 11 loss is 41.412049\n",
      "iter 12 loss is 41.412049\n",
      "iter 13 loss is 41.315959\n",
      "iter 14 loss is 41.315959\n",
      "iter 15 loss is 41.156427\n",
      "iter 16 loss is 40.875113\n",
      "iter 17 loss is 40.875113\n",
      "iter 18 loss is 40.065205\n",
      "iter 19 loss is 39.379616\n",
      "iter 20 loss is 39.379616\n",
      "iter 21 loss is 39.379616\n",
      "iter 22 loss is 39.207450\n",
      "iter 23 loss is 39.207450\n",
      "iter 24 loss is 39.207450\n",
      "iter 25 loss is 39.207450\n",
      "iter 26 loss is 38.401821\n",
      "iter 27 loss is 38.401821\n",
      "iter 28 loss is 38.401821\n",
      "iter 29 loss is 37.537244\n",
      "iter 30 loss is 36.744847\n",
      "iter 31 loss is 36.744847\n",
      "iter 32 loss is 36.744847\n",
      "iter 33 loss is 35.582626\n",
      "iter 34 loss is 35.582626\n",
      "iter 35 loss is 34.292754\n",
      "iter 36 loss is 33.643504\n",
      "iter 37 loss is 33.643504\n",
      "iter 38 loss is 33.643504\n",
      "iter 39 loss is 31.790485\n",
      "iter 40 loss is 31.790485\n",
      "iter 41 loss is 31.790485\n",
      "iter 42 loss is 31.745573\n",
      "iter 43 loss is 31.745573\n",
      "iter 44 loss is 31.184618\n",
      "iter 45 loss is 31.184618\n",
      "iter 46 loss is 30.622561\n",
      "iter 47 loss is 29.298703\n",
      "iter 48 loss is 29.131190\n",
      "iter 49 loss is 27.674251\n",
      "iter 50 loss is 27.674251\n",
      "iter 51 loss is 27.590620\n",
      "iter 52 loss is 27.590620\n",
      "iter 53 loss is 27.590620\n",
      "iter 54 loss is 27.590620\n",
      "iter 55 loss is 27.590620\n",
      "iter 56 loss is 27.477399\n",
      "iter 57 loss is 27.059534\n",
      "iter 58 loss is 26.433575\n",
      "iter 59 loss is 25.392132\n",
      "iter 60 loss is 25.392132\n",
      "iter 61 loss is 25.356451\n",
      "iter 62 loss is 24.816755\n",
      "iter 63 loss is 24.816755\n",
      "iter 64 loss is 24.559875\n",
      "iter 65 loss is 24.559875\n",
      "iter 66 loss is 24.559875\n",
      "iter 67 loss is 24.559875\n",
      "iter 68 loss is 23.664150\n",
      "iter 69 loss is 23.664150\n",
      "iter 70 loss is 23.664150\n",
      "iter 71 loss is 23.664150\n",
      "iter 72 loss is 23.664150\n",
      "iter 73 loss is 23.664150\n",
      "iter 74 loss is 23.664150\n",
      "iter 75 loss is 22.691133\n",
      "iter 76 loss is 22.691133\n",
      "iter 77 loss is 22.691133\n",
      "iter 78 loss is 22.691133\n",
      "iter 79 loss is 21.993747\n",
      "iter 80 loss is 21.803648\n",
      "iter 81 loss is 21.803648\n",
      "iter 82 loss is 21.803648\n",
      "iter 83 loss is 21.803648\n",
      "iter 84 loss is 21.803648\n",
      "iter 85 loss is 21.803648\n",
      "iter 86 loss is 21.630188\n",
      "iter 87 loss is 21.630188\n",
      "iter 88 loss is 21.630188\n",
      "iter 89 loss is 21.630188\n",
      "iter 90 loss is 21.630188\n",
      "iter 91 loss is 21.279636\n",
      "iter 92 loss is 21.279636\n",
      "iter 93 loss is 21.279636\n",
      "iter 94 loss is 21.279636\n",
      "iter 95 loss is 21.279636\n",
      "iter 96 loss is 20.896702\n",
      "iter 97 loss is 20.896702\n",
      "iter 98 loss is 20.459477\n",
      "iter 99 loss is 20.432419\n",
      "accuracy: 0.114500\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(10, 3073) * 0.001 # generate random starting W\n",
    "bestloss = float(\"inf\")\n",
    "for i in range(1000):\n",
    "    step_size = 0.0001\n",
    "    Wtry = W + np.random.randn(10, 3073) * step_size\n",
    "    loss = L(Xtr_cols, Ytr, Wtry)\n",
    "    if loss < bestloss:\n",
    "        W = Wtry\n",
    "        bestloss = loss\n",
    "    print ('iter %d loss is %f' % (i, bestloss))\n",
    "result(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy 3: Following the Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x):\n",
    "    \"\"\" \n",
    "    a naive implementation of numerical gradient of f at x \n",
    "    - f should be a function that takes a single argument\n",
    "    - x is the point (numpy array) to evaluate the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros(x.shape)\n",
    "    h = 0.00001\n",
    "\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        old_value = x[ix]\n",
    "        x[ix] = old_value + h # increment by h\n",
    "        fxh = f(x) # evalute f(x + h)\n",
    "        x[ix] = old_value # restore to previous value (very important!)\n",
    "\n",
    "        # compute the partial derivative\n",
    "        grad[ix] = (fxh - fx) / h # the slope\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c076807271d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3073\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;31m# random weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_numerical_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# get the gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mloss_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# the original loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'original loss: %f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mloss_original\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bc148dc07d38>\u001b[0m in \u001b[0;36meval_numerical_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mold_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m \u001b[0;31m# increment by h\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mfxh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# evalute f(x + h)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_value\u001b[0m \u001b[0;31m# restore to previous value (very important!)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-c076807271d4>\u001b[0m in \u001b[0;36mCIFAR10_loss_fun\u001b[0;34m(W)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# (the weights in our case) so we close over X_train and Y_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mCIFAR10_loss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtr_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3073\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m \u001b[0;31m# random weight vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-567f426ec542>\u001b[0m in \u001b[0;36mL\u001b[0;34m(X, y, W)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# left as exercise to reader in the assignment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# remember interger indexing in python, very important(different from MATLAB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# to use the generic code above we want a function that takes a single argument\n",
    "# (the weights in our case) so we close over X_train and Y_train\n",
    "def CIFAR10_loss_fun(W):\n",
    "    return L(Xtr_cols, Ytr, W)\n",
    "\n",
    "W = np.random.rand(10, 3073) * 0.001 # random weight vector\n",
    "df = eval_numerical_gradient(CIFAR10_loss_fun, W) # get the gradient\n",
    "loss_original = CIFAR10_loss_fun(W) # the original loss\n",
    "print ('original loss: %f' % (loss_original, ))\n",
    "\n",
    "# lets see the effect of multiple step sizes\n",
    "for step_size_log in [10, 9, 8, 7, 6, 5, 4, 3, 2, 1]:\n",
    "    step_size = 10 ** step_size_log\n",
    "    W_new = W - step_size * df # new position in the weight space\n",
    "    loss_new = CIFAR10_loss_fun(W_new)\n",
    "    print ('for step size %f new loss: %f' % (step_size, loss_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vanilla Minibatch Gradient Descent\n",
    "\n",
    "while True:\n",
    "    data_batch = sample_training_data(data, 256) # sample 256 examples\n",
    "    weights_grad = evaluate_gradient(loss_fun, data_batch, weights)\n",
    "    weights += - step_size * weights_grad # perform parameter update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
