{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os;\n",
    "import sys;\n",
    "import pickle;\n",
    "import numpy as np;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def L_i(x, y, W):\n",
    "    \"\"\"\n",
    "    unvectorized version. Compute the multiclass svm loss for a single example (x,y)\n",
    "    - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)\n",
    "    with an appended bias dimension in the 3073-rd position (i.e. bias trick)\n",
    "    - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)\n",
    "    - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)\n",
    "    \"\"\"\n",
    "    delta = 1.0 # see notes about delta later in this section\n",
    "    scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class\n",
    "    correct_class_score = scores[y]\n",
    "    D = W.shape[0] # number of classes, e.g. 10\n",
    "    loss_i = 0.0\n",
    "    for j in xrange(D): # iterate over all wrong classes\n",
    "        if j == y:\n",
    "            # skip for the true class to only loop over incorrect classes\n",
    "            continue\n",
    "        # accumulate loss for the i-th example\n",
    "        loss_i += max(0, scores[j] - correct_class_score + delta)\n",
    "    return loss_i\n",
    "\n",
    "def L_i_vectorized(x, y, W):\n",
    "    \"\"\"\n",
    "    A faster half-vectorized implementation. half-vectorized\n",
    "    refers to the fact that for a single example the implementation contains\n",
    "    no for loops, but there is still one loop over the examples (outside this function)\n",
    "    \"\"\"\n",
    "    delta = 1.0\n",
    "    scores = W.dot(x)\n",
    "    # compute the margins for all classes in one vector operation\n",
    "    margins = np.maximum(0, scores - scores[y] + delta)\n",
    "    # on y-th position scores[y] - scores[y] canceled and gave delta. We want\n",
    "    # to ignore the y-th position and only consider margin on max wrong class\n",
    "    margins[y] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "\n",
    "def L(X, y, W):\n",
    "    \"\"\" \n",
    "    fully-vectorized implementation :\n",
    "    - X holds all the training examples as columns (e.g. 3073 x 50,000 in CIFAR-10)\n",
    "    - y is array of integers specifying correct class (e.g. 50,000-D array)\n",
    "    - W are weights (e.g. 10 x 3073)\n",
    "    \"\"\"\n",
    "    # evaluate loss over all examples in X without using any for loops\n",
    "    # left as exercise to reader in the assignment\n",
    "    delta = 1.0\n",
    "    scores = W.dot(X)\n",
    "    num = scores.shape[1]\n",
    "    # remember interger indexing in python, very important(different from MATLAB)\n",
    "    margins = np.maximum(0, scores - scores[y,np.arange(num)] + delta); \n",
    "    margins[y,np.arange(num)] = 0\n",
    "    loss_i = np.sum(margins)\n",
    "    return loss_i\n",
    "def regulizaion(W):\n",
    "    londa = 1.0\n",
    "    rW=np.sum(np.square(W,W))\n",
    "    return rW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.70816431  0.03442109  0.17010297  0.90296702  0.29542718]\n",
      " [ 0.25466689  0.68703178  0.26016867  0.66361788  0.50326851]\n",
      " [ 0.10300962  0.16751117  0.50849381  0.27337434  0.02334002]] \n",
      " [7 7 2 9 2] \n",
      " [[ 0.11872983  0.19310517  0.58068263]\n",
      " [ 0.01050618  0.75176593  0.91528878]\n",
      " [ 0.21047062  0.03868682  0.62935613]\n",
      " [ 0.74527119  0.53997264  0.57182823]\n",
      " [ 0.91526212  0.79915543  0.17714735]\n",
      " [ 0.66452543  0.96679452  0.94113623]\n",
      " [ 0.49490059  0.6457706   0.92826558]\n",
      " [ 0.89506545  0.7930193   0.58328426]\n",
      " [ 0.48554148  0.22690476  0.28560906]\n",
      " [ 0.86400159  0.67749893  0.85137499]]\n",
      "[[  1.40967714e-02   3.72896084e-02   3.37192317e-01]\n",
      " [  1.10379795e-04   5.65152015e-01   8.37753559e-01]\n",
      " [  4.42978812e-02   1.49666996e-03   3.96089136e-01]\n",
      " [  5.55429146e-01   2.91570447e-01   3.26987520e-01]\n",
      " [  8.37704750e-01   6.38649405e-01   3.13811843e-02]\n",
      " [  4.41594042e-01   9.34691644e-01   8.85737403e-01]\n",
      " [  2.44926593e-01   4.17019666e-01   8.61676978e-01]\n",
      " [  8.01142164e-01   6.28879617e-01   3.40220529e-01]\n",
      " [  2.35750530e-01   5.14857719e-02   8.15725329e-02]\n",
      " [  7.46498750e-01   4.59004796e-01   7.24839369e-01]] \n",
      " 12.7702411753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.770241175319351"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=np.random.rand(3,5)\n",
    "y=np.random.randint(10, size=5)\n",
    "W=np.random.rand(10,3)\n",
    "print(X,\"\\n\",y,\"\\n\",W)\n",
    "L(X, y, W)\n",
    "regulizaion(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
